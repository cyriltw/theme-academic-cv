@misc{weerasooriya_vicarious_2023,
 abstract = {This paper examines social web content moderation from two key perspectives: automated methods (machine moderators) and human evaluators (human moderators). We conduct a noise audit at an unprecedented scale using nine machine moderators trained on well-known offensive speech data sets evaluated on a corpus sampled from 92 million YouTube comments discussing a multitude of issues relevant to US politics. We introduce a first-of-its-kind data set of vicarious offense. We ask annotators: (1) if they find a given social media post offensive; and (2) how offensive annotators sharing different political beliefs would find the same content. Our experiments with machine moderators reveal that moderation outcomes wildly vary across different machine moderators. Our experiments with human moderators suggest that (1) political leanings considerably affect first-person offense perspective; (2) Republicans are the worst predictors of vicarious offense; (3) predicting vicarious offense for the Republicans is most challenging than predicting vicarious offense for the Independents and the Democrats; and (4) disagreement across political identity groups considerably increases when sensitive issues such as reproductive rights or gun control/rights are discussed. Both experiments suggest that offense, is indeed, highly subjective and raise important questions concerning content moderation practices.},
 author = {Weerasooriya, Tharindu Cyril and Dutta, Sujan and Ranasinghe, Tharindu and Zampieri, Marcos and Homan, Christopher M. and KhudaBukhsh, Ashiqur R.},
 copyright = {All rights reserved},
 keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
 month = {February},
 note = {arXiv:2301.12534 [cs]},
 publisher = {arXiv},
 title = {Vicarious Offense and Noise Audit of Offensive Speech Classifiers},
 url = {http://arxiv.org/abs/2301.12534},
 urldate = {2023-02-25},
 year = {2023}
}
