@inproceedings{homanAnnotatorResponseDistributions2022,
 abstract = {Annotator disagreement is often dismissed as noise or the result of poor annotation process quality. Others have argued that it can be meaningful. But lacking a rigorous statistical foundation, the analysis of disagreement patterns can resemble a high-tech form of tea-leaf-reading. We contribute a framework for analyzing the variation of per-item annotator response distributions to data for humans-in-the-loop machine learning. We provide visualizations for, and use the framework to analyze the variance in, a crowdsourced dataset of hard-to-classify examples of the OpenImages archive.},
 author = {Homan, Christopher and Weerasooriya, Tharindu Cyril and Aroyo, Lora and Welty, Chris},
 booktitle = {Proceedings of the 1st Workshop on Perspectivist Approaches to NLP @LREC2022},
 language = {en},
 pages = {10},
 publisher = {European Language Resources Association},
 title = {Annotator Response Distributions as a Sampling Frame},
 url = {http://lrec-conf.org/proceedings/lrec2022/workshops/NLPerspectives/pdf/2022.nlperspectives-1.8.pdf},
 year = {2022}
}
